{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import std libraries\n",
    "import os, sys\n",
    "import csv\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model from training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        - label \"cell_id\" + list of feature labels\n",
    "        - cell_id + vector of features + class\n",
    "    \"\"\"\n",
    "    cell_id_and_features_labels = []\n",
    "    cell_counts = []\n",
    "    with open(file_path) as csvfile:\n",
    "        # read the file into rows\n",
    "        rows = csv.reader(csvfile, delimiter='\\t')\n",
    "\n",
    "        # get labels\n",
    "        count = 0\n",
    "        for row in rows:\n",
    "            if count == 0:\n",
    "                cell_id_and_features_labels = row\n",
    "            else:\n",
    "                cell_counts.append(row)\n",
    "            count += 1\n",
    "\n",
    "    return cell_id_and_features_labels[:-1], cell_counts\n",
    "\n",
    "def extract_features_and_classes(file_path):\n",
    "    \"\"\"\n",
    "    Read the CSV at file_path\n",
    "    and returns X and y\n",
    "    \"\"\"\n",
    "    # contains the \"cell_id\" label + all the other feature labels\n",
    "    cell_id_and_features_labels = []\n",
    "\n",
    "    # contains the row\n",
    "    # each row represents the features vector of a cell\n",
    "    # x[i] where x is the features vector is the count of how many POIs\n",
    "    # there are in that cell\n",
    "    cell_counts = []\n",
    "\n",
    "    cell_id_and_features_labels, cell_counts = read_from_csv(file_path)\n",
    "\n",
    "    training_classes = [x[-1] for x in cell_counts]\n",
    "    training_classes_without_duplicates = list(set(training_classes))\n",
    "    \n",
    "# DEBUG:    print(\"number of features\", len(cell_id_and_features_labels) - 1)\n",
    "# DEBUG:   print(\"number of classes\", len(training_classes_without_duplicates))   \n",
    "    \n",
    "    # from each row\n",
    "    xs = [x[1:-1] for x in cell_counts] # remove cell_id and y\n",
    "\n",
    "    # note, first element of xs corresponds to first in ys and so on\n",
    "    # this because with list comprehension, order is preserved\n",
    "\n",
    "    # note that, we need that all elements in xs are integers\n",
    "    # and, because we read it from CSV actually are strings\n",
    "    # to convert them\n",
    "    xs = [list(map(int, x)) for x in xs]\n",
    "\n",
    "    # convert xs and ys to numpy arrays    \n",
    "    X = np.array(xs)\n",
    "    y = training_classes\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training file\n",
    "training_file_path = '../../../data/w2v_urban/mdetail/baseline/training200.csv'\n",
    "X_train, y_train = extract_features_and_classes(training_file_path)\n",
    "\n",
    "test_file_path = '../../../data/w2v_urban/mdetail/baseline/test200.csv'\n",
    "X_test, y_test = extract_features_and_classes(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import itertools\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "def divide_sets_by_index(dataset, training_index, test_index):\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        if i in training_index:\n",
    "            training_set.append(dataset[i])\n",
    "        else:\n",
    "            test_set.append(dataset[i])\n",
    "    \n",
    "    return training_set, test_set\n",
    "\n",
    "def inner_cross_process(combination_parameters, X_train, y_train):\n",
    "    cvalue, gammavalue = combination_parameters\n",
    "\n",
    "    innerf1 = []\n",
    "    skf = model_selection.StratifiedKFold(n_splits=5, random_state=1234, shuffle=True)\n",
    "    \n",
    "    for train_indexes, test_indexes in skf.split(X_train, y_train):\n",
    "\n",
    "        X2_train, X2_test = divide_sets_by_index(X_train, train_indexes, test_indexes)\n",
    "        y2_train, y2_test = divide_sets_by_index(y_train, train_indexes, test_indexes)\n",
    "\n",
    "        sclf = SVC(C=cvalue, kernel='rbf', gamma=gammavalue)\n",
    "        sclf.fit(X2_train, y2_train)\n",
    "\n",
    "        ipred = sclf.predict(X2_test)\n",
    "        innerf1.append(metrics.f1_score(ipred, y2_test, average=\"macro\"))\n",
    "                         \n",
    "    return combination_parameters, np.mean(innerf1)    \n",
    "\n",
    "def innerFoldCrossValidation(X_train, y_train):\n",
    "    \n",
    "    # Values of C which I have to test\n",
    "    Cvalues = [1e-02, 1e-01, 1e00, 1e01, 1e02]\n",
    "    \n",
    "    # Values of Gamma which I have to test\n",
    "    Gammavalues = [1e-02, 1e-01, 1e00, 1e01, 1e02]\n",
    "    \n",
    "    # Get the combination of Cs and Gamma parameters\n",
    "    combination_parameters = list(itertools.product(Cvalues, Gammavalues))\n",
    "    \n",
    "    with Pool(processes=4) as pool:\n",
    "        combination_and_scores = pool.map(partial(inner_cross_process, X_train=X_train, y_train=y_train), combination_parameters)\n",
    "\n",
    "    assert len(combination_and_scores) == len(combination_parameters)\n",
    "    \n",
    "    max_score = 0\n",
    "    best_combination = None\n",
    "    \n",
    "    for combination, score in combination_and_scores:\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best_combination = combination\n",
    "            \n",
    "    return best_combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold -  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold -  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold -  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold -  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold -  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/emavgl/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "final_accuracy = []\n",
    "final_f1 = []\n",
    "counter = 1\n",
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=5, random_state=1234, shuffle=True)\n",
    "for train_indexes, test_indexes in skf.split(X_train, y_train):\n",
    "    print(\"Fold - \", counter)\n",
    "    counter += 1\n",
    "    \n",
    "    X2_train, X2_test = divide_sets_by_index(X_train, train_indexes, test_indexes)\n",
    "    y2_train, y2_test = divide_sets_by_index(y_train, train_indexes, test_indexes)\n",
    "    \n",
    "    best_c, best_gamma = innerFoldCrossValidation(X2_train, y2_train)\n",
    "    clf = SVC(C=best_c, kernel='rbf', gamma=best_gamma)\n",
    "    model = clf.fit(X2_train, y2_train)\n",
    "    results = model.predict(X2_test)\n",
    "    \n",
    "    acc = accuracy_score(y2_test, results)\n",
    "    f1 = f1_score(y2_test, results, average=\"macro\")\n",
    "    \n",
    "    final_accuracy.append(acc)\n",
    "    final_f1.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.578967119528\n",
      "f1 0.473802859186\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy\", np.mean(final_accuracy))\n",
    "print(\"f1\", np.mean(final_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
